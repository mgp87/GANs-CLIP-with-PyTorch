# -*- coding: utf-8 -*-
"""CLIP_MultiModal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H5PmGO_lIDprBeKZrNm0ZrmboXkttY-p
"""

!git clone https://github.com/openai/CLIP.git
!git clone https://github.com/CompVis/taming-transformers

# Install libraries
!pip install --no-deps ftfy regex tqdm
!pip install omegaconf pytorch-lightning
!pip uninstall torchtext --yes
!pip install einops
# !pip install git+https://github.com/openai/CLIP.git
# !pip install git+https://github.com/CompVis/taming-transformers.git
# !pip install omegaconf==2.0.0 pytorch-lightning==1.0.0 torch==1.13.1 torchvision

# Import libraries
import numpy as np
import torch, os, imageio, pdb, math
import torchvision
import torchvision.transforms as T
import torchvision.transforms.functional as TF
import PIL
import matplotlib.pyplot as plt
import yaml
from omegaconf import OmegaConf
from CLIP import clip

# remove warnings from code

# import warnings
# warnings.filterwarnings('ignore')

# Utils

# Show image from tensor format
def img_from_tensor(tensor):
  img=tensor.clone()
  img=img.mul(255).byte()
  img=img.cpu().numpy().transpose((1,2,0))
  plt.figure(figsize=(10,10))
  plt.axis('off')
  plt.imshow(img)
  plt.show()

def normalize_data(data):
  return(data.clip(-1, 1) + 1)/2

# Hyperparams
learning_rate=0.5
batch_size=1
weight_decay=0.1 # regularization param for genralization improvement
noise_factor=0.2 # Improves crop
total_iterations=400 # Usually the more, the better detail (and slower)
img_shape=[600, 600, 3] # [height, width, channels (RGB)]
size_h, size_w, channels = img_shape
device='cuda' if torch.cuda.is_available() else 'cpu'

# CLIP Model Block
clip_model, _ = clip.load('ViT-B/32', jit=False) #
clip_model.eval() # it's pre-trained, we just want to generate
print(clip.available_models())
print("Clip model input resolution: ", clip_model.visual.input_resolution)
if device == 'cuda':
  torch.cuda.empty_cache()

"""### We will need to resize input images to 224"""

# Taming Transformers (already trained too)
# %cd taming-transformers/
# %pwd
# !mkdir -p models/vqgan_imagenet_f16_16384/checkpoints
# !mkdir -p models/vqgan_imagenet_f16_16384/configs

# if len(os.listdir('models/vqgan_imagenet_f16_16384/checkpoints/')) == 0:
  # download models
  # !wget 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' -O 'models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt'
  # !wget 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' -O 'models/vqgan_imagenet_f16_16384/configs/model.yaml'

# Commented out IPython magic to ensure Python compatibility.
# %cd taming-transformers/
from taming.models.vqgan import VQModel

def load_config(path, display=False):
  conf_data=OmegaConf.load(path)
  if display:
    print(yaml.dump(OmegaConf.to_container(conf_data)))
  return conf_data

def load_vqgan(config, checkpoint_path=None):
  model=VQModel(**config.model.params)
  if checkpoint_path is not None:
    state_dict = torch.load(checkpoint_path, map_location="cpu")["state_dict"]
    missing, unexpected = model.load_state_dict(state_dict, strict=False)
  return model.eval()

def generator(x):
  x = taming_model.post_quant_conv(x)
  x = taming_model.decoder(x)
  return x

taming_conf = load_config("/content/drive/MyDrive/CLIP_Taming/model.yaml", display=True)
taming_model = load_vqgan(taming_conf, checkpoint_path="/content/drive/MyDrive/CLIP_Taming/last.ckpt").to(device)

# Declare values to be optimized
class Params(torch.nn.Module):
  def __init__(self):
    super(Params, self).__init__()
    self.data=0.5*torch.randn(batch_size, 256, size_h//16, size_w).cuda() # 256 channels due to arch constraints
    self.data=torch.nn.Parameter(torch.sin(self.data)) # for positional embedding

  def forward(self):
    return self.data

def init_params():
  params=Params().cuda()
  optimizer=torch.optim.AdamW([{'params':[params.data], 'lr': learning_rate}], weight_decay=weight_decay)
  return params, optimizer

# Encoding prompts
normalize = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))
def encode_text(text):
  tokenized = clip.tokenize(text).cuda()
  tokenized=clip_model.encode_text(tokenized).detach().clone()
  return tokenized

def create_encodings(include, exclude, extras): # things to be in the pic, things to omit in the pic, characteristics to add to included things
  include_encodings=[]
  for text in include:
    include_encodings.append(encode_text(text))
  exclude_encodings=encode_text(exclude) if exclude != '' else 0
  extras_encodings=encode_text(extras) if extras != '' else 0
  return include_encodings, exclude_encodings, extras_encodings

augmentation_transform = torch.nn.Sequential(
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.RandomAffine(30, (0.2, 0.2), fill=0)
).cuda()

Params, optimizer = init_params()

# Test
with torch.no_grad():
  print(Params().shape)
  image = normalize_data(generator(Params()).cpu())
  print("image dimensions --> ", image.shape)
  img_from_tensor(image[0])

# Crops
def make_crops(image, n_crops=30):
  p=size_h // 2
  image = torch.nn.functional.grad(image, (p,p,p), mode='constant', value=0)
  image = augmentation_transform(image)
  crop_set = []
  for c in range(n_crops):
    gap_1 = int(torch.normal(1.3, 0.3, ()).clip(0.37, 1.95) * size_h)
    gap_2 = int(torch.normal(1.2, 0.5, ()).clip(0.29, 1.61) * size_h)

    offset_x = torch.randint(0, int(size_h*2 - gap_1), ())
    offset_y = torch.randint(0, int(size_h*2 - gap_1), ())

    crop=image[:, :, offset_x:offset_x+gap_2, offset_y:offset_y+gap_2]
    crop = torch.nn.functional.interpolate(crop, (224, 224), mode='bilinear', align_content=True)
    crop_set.append(crop)

  image_crop = torch.cat(crop_set, 0)
  randnormal = torch.randn_like(image_crop, requires_grad=False)
  n_rands=2
  rands_total = torch.rand((image_crop.shape[0], 1, 1, 1)).cuda()

  for n in range(n_rands):
    rands_total*=torch.rand((image_crop.shae[0],1,1,1)).cuda()

  image_crop = image_crop + noise_factor * rands_total*randnormal
  return image_crop

# show current state
def show_state(Params, show_crop):
  generated = generator(Params())
  if(show_crop):
    print("Augmented crop example")
    augmented_generated = generated.float()
    autmented_generated = make_crops(augmented_generated, n_crops=1)
    augmented_gen_normalized=normalize_data(augmented_generated[0])
    img_from_tensor(augmented_gen_normalized)

  print("Generate")
  latest_generated=normalize_data(generated.cpu())
  img_from_tensor(latest_generated[0])

  return(latest_generated[0])

# Optimization
def optimize_result(Params, prompt):
  alpha=1 # weight of included encodings
  beta=0.5 # weight of excluded encodings

  # Image encoding
  output = generator(Params)
  output = normalize_data(output)
  output = make_crops(output)
  output = normalize(output)
  image_encodings = clip_model.encode_image(output)

  # Text encoding
  encoding = weight_1*prompt + weight_2*extras_encodings
  text_included_encodings = encoding / encoding.normalize(dim=-1, keepdim=True)
  text_excluded_encodings = excluded_encodings

  # Loss Calc
  main_loss = torch.cosine_similarity(text_included_encodings, image_encodings, -1)
  penalize_loss = torch.cosine_similarity(text_excluded_encodings, image_encodings, -1)
  total_loss = -alpha*main_loss + beta*penalize_loss

  return total_loss


def optimize(Params, optimizer, prompt):
  loss = optimize_result(Params, prompt).mean()
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
  return loss

# Training loop
def training_loop(Params, optimizer, show_crop=False):
  result_images=[]
  result_z = []

  for prompt in included_encodings:
    iteration=0
    Params, optimizer = init_params()

    for iteration in range(total_iterations):
      loss = optimize(Params, optimizer, prompt)
      if iteration > 0 and iteration % (total_iterations - 1) == 0:
        new_image = show_state(Params, show_crop)
        result_images.apped(new_image)
        result_z.append(Params())
        print("Loss: ", loss.item(), "\niteration:", iteration)

      iteration += 1
    torch.cuda.empty_cache()
  return result_images, result_z

torch.cuda.empty_cache()
included_text=["sketch of a woman with an umbrella", "sketch of a man with a hat"]
excluded_text="watermark, cropped, confusing, blurry"
extras = "watercolor paper texture"
weight_1 = 1
weight_2 = 1

included_encodings, excluded_encodings, extras_encodings = create_encodings(included_text, excluded_text, extras)
result_image, result_z = training_loop(Params, optimizer, show_crop=True)

print(len(result_image), len(result_z))
print(result_image[0].shape, result_z[0].shape)
print(result_z[0].max(), result_z[0].min())

def interpolate(result_z_list, duration_list):
  generated_img_list=[]
  fps=25

  for i, (z, duration) in enumerate(zip(result_z_list, duration_list)):
    n_steps = int(duration*fps)
    z0=z
    z1=result_z_list[(i+1)%len(result_z_list)]

    for step in range(n_steps):
      alpha = math.sin(1.5*step/n_steps)**3
      z_new = alpha*z1 + (1 - alpha) * z0

      new_generated=normalize_data(generator(z_new).cpu())[0]
      new_image = T.toPILImage(mode='RGB')(new_generated)
      generated_img_list.append(new_image)

  return generated_img_list

  durations=[3,3,3,3,3,3]
  interpolated_result = interpolate(result_z, durations)

# Making of a Video
output_video_path=f"/content/result.mp4"
writer = imageio.get_writer(output_video_path, fps=25)
for pil_img in interpolated_result:
  img = np.array(pil_img, dtype=np.uint8)
  writer.append_data(img)

writer.close()

from IPython.display import HTML
from base64 import b64encode

mp4 = open("/content/result.mp4", "rb").read()
data = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML("""<video width=800 controls><source src="%s" type="video/mp4"></video>""" % data)