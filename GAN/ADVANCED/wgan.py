# -*- coding: utf-8 -*-
"""WGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fEMI0pY7nZSHoN7zbGDheHSlr89B4QtI
"""

import torch, torchvision, os, PIL, pdb
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torchvision.utils import make_grid
from tqdm.auto import tqdm
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

def show(tensor, n=25, wandbactive=0, name=''):
  data = tensor.detach().cpu()
  grid = make_grid(data[:n], nrow=5).permute(1,2,0)

  # Optional: wandb.ai --> you need to create an account and get your API if planning to use it
  # Displays images remotely and uses wandb activation

  if wandbactive==1:
    wandb.log({name:wandb.Image(grid.numpy().clip(0,1))})

  plt.imshow(grid.clip(0,1))
  plt.show()

# Hyperparameters and general params
epochs=10000
batch_size=128
learning_rate=1e-4
d_z=200
device='cuda' if torch.cuda.is_available() else 'cpu'

current_step=0
critic_cycles=5
generator_losses=[]
critic_losses=[]
show_step=35
save_step_checkpoint=35

wandbact=1 # track stats through weights and biases --> OPTIONAL

# Optional installation
!pip install wandb -qqq
import wandb

from google.colab import userdata
wandb_key = userdata.get("WANDB_API")

wandb.login(key=wandb_key)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# experiment_name=wandb.util.generate_id()
# myrun=wandb.init(
#     project="wgan",
#     group=experiment_name,
#     config={
#         "optimizer":"adam",
#         "model":"wgan gp",
#         "epoch":"1000",
#         "batch_size":128
#     }
# )
# config=wandb.config
# print(experiment_name) # Run ID

class Generator(nn.Module):
  def __init__(self, d_z=64, d_d=16):
    super(Generator, self).__init__()
    self.d_z = d_z

    # Convolution layers instead of linear layers
    self.gen = nn.Sequential(
        # 1x1 image with d_z number of channels (200)
        # Calculations from the formula: (n-1)*stride - 2 * padding +ks, resulting: 4, 8, 16
        nn.ConvTranspose2d(d_z, d_d * 32, 4, 1, 0), # 4x4 (ch: 200 --> 512)
        nn.BatchNorm2d(d_d * 32),
        nn.ReLU(True),

        nn.ConvTranspose2d(d_d * 32, d_d * 16, 4, 2, 1), # 8x8 (ch: 512 --> 256)
        nn.BatchNorm2d(d_d * 16),
        nn.ReLU(True),

        nn.ConvTranspose2d(d_d * 16, d_d * 8, 4, 2, 1), # 16x16 (ch: 256 --> 128)
        nn.BatchNorm2d(d_d * 8),
        nn.ReLU(True),

        nn.ConvTranspose2d(d_d * 8, d_d * 4, 4, 2, 1), # 32x32 (ch: 128 --> 64)
        nn.BatchNorm2d(d_d * 4),
        nn.ReLU(True),

        nn.ConvTranspose2d(d_d * 4, d_d * 2, 4, 2, 1), # 64x64 (ch: 64 --> 32)
        nn.BatchNorm2d(d_d * 2),
        nn.ReLU(True),

        nn.ConvTranspose2d(d_d * 2, 3, 4, 2, 1), # 128x128 (ch: 32 --> 3)
        nn.Tanh(), # Result from -1 to 1
    )

  def forward(self, noise):

    x=noise.view(len(noise), self.d_z, 1, 1) # 128 x 200 x 1 x 1
    return self.gen(x)

def generate_noise(n, d_z, device='cuda'):
  return torch.randn(n, d_z, device=device)

# Critic model
class Critic(nn.Module):
  def __init__(self, d_d=16):
    super(Critic, self).__init__()
    self.critic = nn.Sequential(
        nn.Conv2d(3, d_d, 4, 2, 1), # 3 channels --> RGB | (n+2*pad - ks)//stride +1 = (128+2*1-4)//2+1 = 64 (ch: 3 --> 16)
        nn.InstanceNorm2d(d_d), # Normalizing by instance instead of channel or batch, works best for critic
        nn.LeakyReLU(0.2),

        nn.Conv2d(d_d, d_d * 2, 4, 2, 1),
        nn.InstanceNorm2d(d_d * 2), # 32x32 (ch: 16 --> 32)
        nn.LeakyReLU(0.2),

        nn.Conv2d(d_d * 2, d_d * 4, 4, 2, 1),
        nn.InstanceNorm2d(d_d * 4), # you 16x16 (ch: 32 --> 64)
        nn.LeakyReLU(0.2),

        nn.Conv2d(d_d * 4, d_d * 8, 4, 2, 1),
        nn.InstanceNorm2d(d_d * 8), # you 8x8 (ch: 64 --> 128)
        nn.LeakyReLU(0.2),

        nn.Conv2d(d_d * 8, d_d * 16, 4, 2, 1),
        nn.InstanceNorm2d(d_d * 16), # you 4x4 (ch: 128 --> 256)
        nn.LeakyReLU(0.2),

        nn.Conv2d(d_d * 16, 1, 4, 1, 0), # 1x1 image output (ch: 256 --> 1)
    )

  def forward(self, image):
    # image: 128 x 3 x 128 x 128
    critic_pred = self.critic(image) # 128 x 1 x 1 x 1
    return critic_pred.view(len(critic_pred), -1) # 128 x 1

# Optional way to initialize params in Pytorch
def init_weights(w):
  if isinstance(w, nn.Conv2d) or isinstance(w, nn.ConvTranspose2d):
    torch.nn.init.normal_(m.weight, 0.0, 0.02)
    torch.nn.init.constant_(w.bias, 0)

  if isinstance(w, nn.BatchNorm2d):
    torch.nn.init.normal_(w.weight, 0.0, 0.02)
    torch.nn.init.constant_(w.bias, 0)

# gen=gen.apply(init_weights)
# critic=critic.apply(init_weights)

# Download Dataset --> CelebA dataset:
# https://www.kaggle.com/datasets/jessicali9530/celeba-dataset
# https://www.kaggle.com/datasets/jessicali9530/celeba-dataset/download?datasetVersionNumber=2

# https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html
# https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8?resourcekey=0-5BR16BdXnb8hVj6CNHKzLg&usp=sharing

# import gdown, zipfile, requests

# url = 'https://drive.google.com/file/d/0B7EVK8r0v71pZjFTYXZWM3FlRnM/view?usp=drive_link&resourcekey=0-dYn9z10tMJOBAkviAcfdyQ'
# path='data/celebA'
# download_path=f'{path}/celebA.zip'

# if not os.path.exists(path):
#   os.makedirs(path)

# gdown.download(url, download_path, quiet=False)

# response = requests.get(url, stream=True)

# with open(download_path, mode="wb") as file:
  # for chunk in response.iter_content(chunk_size=10 * 1024):
    # file.write(chunk)

# Use Google Drive :S

import zipfile

with zipfile.ZipFile('/content/drive/MyDrive/WGAN_CelebA/celebA.zip', 'r') as zip:
  zip.extractall('/content/drive/MyDrive/WGAN_CelebA/celebA')

# Dataset, DataLoader, declare generator, critic, test dataset

class Dataset(Dataset):
  def __init__(self, path, size=128, lim=10000):
    self.sizes=[size, size]
    items, labels=[], []

    for data in os.listdir(path)[:lim]:
      item = os.path.join(path, data)
      # print(item)
      items.append(item)
      labels.append(data)

    self.items=items
    self.labels=labels

  def __len__(self):
    return len(self.items)

  def __getitem__(self, idx):
    data = PIL.Image.open(self.items[idx]).convert("RGB") # 128 x 128
    data = np.asarray(torchvision.transforms.Resize(self.sizes)(data)) # 128 x 128 x 3
    data = np.transpose(data, (2, 0, 1)).astype(np.float32, copy=False) # 3 x 128 x 128 from 0 to 255
    data = torch.from_numpy(data).div(255) # np array to tensor with values from 0 to 1
    return data, self.labels[idx]

## Dataset instance
data_path='/content/drive/MyDrive/WGAN_CelebA/celebA/img_align_celeba/img_align_celeba'
dataset = Dataset(data_path, size=128, lim=5000)

# DataLoader
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Models
generator = Generator(d_z).to(device)
critic = Critic().to(device)

# Optimizers
generator_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))
critic_optimizer = torch.optim.Adam(critic.parameters(), lr=learning_rate, betas=(0.5, 0.9))

# wandb optional for monitoring
if wandbact == 1:
  wandb.watch(generator, log_freq=100)
  wandb.watch(critic, log_freq=100)

x, y = next(iter(dataloader))
show(x)

from google.colab import drive
drive.mount('/content/drive')

# Gradient Penalty
def gradient_penalty(real_img, fake_img, critic, alpha, gamma=10):
  mixed_img = real_img * alpha + fake_img * (1-alpha) # Linear interpolation - 128 x 3 x 128 x 128
  mixed_scores = critic(mixed_img) # 128 x 1
  gradient = torch.autograd.grad(
      inputs = mixed_img,
      outputs = mixed_scores,
      grad_outputs = torch.ones_like(mixed_scores),
      retain_graph=True,
      create_graph=True
  )[0] # 128 x 3 x 128 x 128

  gradient = gradient.view(len(gradient), -1) # 128 x 49152
  gradient_norm = gradient.norm(2, dim=1) # L2 norm applied to dim 1 (49152)
  gp = gamma * ((gradient_norm - 1)**2).mean()
  return gp

# Load and Save checkpoints
root_path='/content/drive/MyDrive/WGAN_CelebA/celebA/'
def save_checkpoint(name):
  torch.save({
    'epoch':epoch,
    'model_state_dict': generator.state_dict(),
    'optimizer_state_dict': generator_optimizer.state_dict()
  }, f"{root_path}G-{name}.pkl")

  torch.save({
    'epoch':epoch,
    'model_state_dict': critic.state_dict(),
    'optimizer_state_dict': critic_optimizer.state_dict()
  }, f"{root_path}C-{name}.pkl")

  print("Saved checkpoint!")

def load_checkpoint(name):
  checkpoint = torch.load(f"{root_path}G-{name}.pkl")
  generator.load_state_dict(checkpoint['model_state_dict'])
  generator_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

  checkpoint = torch.load(f"{root_path}C-{name}.pkl")
  critic.load_state_dict(checkpoint['model_state_dict'])
  critic_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

  print("Loaded checkpoint!")

# Training Loop
for epoch in range(epochs):
  for real, _ in tqdm(dataloader):
    current_bs = len(real) # 128
    real = real.to(device)

    # Critic
    mean_critic_loss = 0
    for _ in range(critic_cycles):
      critic_optimizer.zero_grad()
      noise=generate_noise(current_bs, d_z)
      fake = generator(noise)
      critic_fake_pred = critic(fake.detach())
      critic_real_pred = critic(real)

      # Gradient penalty
      alpha = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True) # 128 x 1 x1 x1
      gp = gradient_penalty(real, fake.detach(), critic, alpha)

      critic_loss = critic_fake_pred.mean() - critic_real_pred.mean() + gp
      mean_critic_loss += critic_loss.item() / critic_cycles

      critic_loss.backward(retain_graph=True)
      critic_optimizer.step()

    critic_losses += [mean_critic_loss]

    # Generator
    generator_optimizer.zero_grad()
    noise = generate_noise(current_bs, d_z)
    fake = generator(noise)
    critic_fake_pred = critic(fake)

    generator_loss = -critic_fake_pred.mean()
    generator_loss.backward()
    generator_optimizer.step()

    generator_losses += [generator_loss.item()]

    # Stats
    if (wandbact==1):
      wandb.log({"Epoch": epoch, "Step": current_step, "Critic loss":mean_critic_loss, "Generator loss": generator_loss})

    # For demo purposes I will overwrite checkpoints
    # It's better to save a different one each time because sometimes the latest is not the best result
    if current_step % save_step_checkpoint == 0 and current_step > 0:
      print("Save checkpoint: ", current_step, save_step_checkpoint)
      save_checkpoint("Latest")

    if current_step % show_step == 0 and current_step > 0:
      show(fake, wandbactive=1, name="fake")
      show(real, wandbactive=1, name="real")

      generator_mean = sum(generator_losses[-show_step:]) / show_step
      critic_mean = sum(critic_losses[-show_step:]) / show_step

      print(f"Epoch: {epoch}, Step: {current_step}, Generator Loss: {generator_mean}, Critic Loss: {critic_mean}")

      plt.plot(
          range(len(generator_losses)),
          torch.Tensor(generator_losses),
          label="Generator Loss"
      )

      plt.plot(
          range(len(generator_losses)),
          torch.Tensor(critic_losses),
          label="Critic Loss"
      )

      plt.ylim(-150, 150)
      plt.legend()
      plt.show()
    current_step += 1

# 10000 / 128 = 78.125 (79 steps per epoch)
# Stopped at Epoch 85

# Generate new pics (faces)
noise = generate_noise(batch_size, d_z)
generated_image = generator(noise)
show(generated_image)

plt.imshow(generated_image[4].detach().cpu().permute(1,2,0).squeeze().clip(0,1))

# Morphing: interpolation between points in latent space
from mpl_toolkits.axes_grid1 import ImageGrid

generated_set=[]
z_shape=[1, 200, 1, 1]
rows=4
steps=17

for i in range(rows):
  z1, z2 = torch.randn(z_shape), torch.randn(z_shape)
  for alpha in np.linspace(0, 1, steps):
    z = alpha * z1 + (1 - alpha) * z2
    result = generator(z.cuda())[0]
    generated_set.append(result)

fig = plt.figure(figsize=(25, 11))
grid = ImageGrid(fig, 111, nrows_ncols=(rows, steps), axes_pad=0.1)

for ax, img in zip (grid, generated_set):
  ax.axis('off')
  result=img.cpu().detach().permute(1, 2, 0)
  result = result - result.min()
  result = result / (result.max() - result.min())
  ax.imshow(result.clip(0,1.0))

plt.show()

